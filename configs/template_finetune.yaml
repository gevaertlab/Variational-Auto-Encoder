# template fo the configuration file.Some params are required. They must be specified
# Others are optional. If blank, will use the default value in template file.
pretrain_ckpt_load_dir: required
  # log directory of the trained model, will load from here

model_params: 
  # used to initialize model instance

  name: required 
    # e.g. "VAE3D", used to call the correct model instance,
    # should be registered in models.__init__.py
  in_channels: required
    # e.g. 1, if black and white image, then 1, if RGB, then 3
  latent_dim: required
    # e.g. 1024, the dimension of the latent vector (mean, std)
  beta: optional 1
    # used in betaVAE, the ratio of KL div loss v.s. the recon loss
  hidden_dims: required
    # e.g. [4, 16, 32, 64]
    # only four layer because the size of the patch is changed
    # compression rate = 1/32 = 2 ** 5
    # now: *4->1, *4->1/2, *4->1/2, *4->1/4, *4=1/2
    # information shrink level: [1, 1, 1, 1, 1]
    # only four layer because the size of the patch is changed
    # NOTE: can only grow at rate of 8 *, otherwise the information won't be compressed
    # the final parameter size = hidden_dims[-1] * 2**3
    # e.g. 64 * 8 = 512, the latent_dim should be <= this number


exp_params:
  # parameters for experiment (pytorch_lightning module)

  dataset: optional LIDCPatch32Dataset
    # dataset that the model will load and train on, should be 
    # registered in dataset.__init__.py
  root_dir: optional None
    # root_dir of the dataset, sometimes the dataset object will automatically set that
  LR: required 
    # e.g. 0.001
  batch_size: required
    # e.g. 36, image batch size
  vis_batch_size: optional 36
    # batch size for visualization of images
  max_lr: optional 0.05
    # used in OneCycleLR to fasten training
  final_div_factor: optional 10000
    # used in OneCycleLR to fasten training
  early_stopping: optional False
    # whether to use early stopping during training


trainer_params:
  # params for training, used to initialize trainer

  max_epochs: optional
    # also ok if specify max_steps, pytorch lightning's trainer will default epochs
    # refer to https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#max-epochs
    # won't require to provide here as pytorch lightning handles this well
  max_steps: optional
  check_val_every_n_epoch: optional 10
    # how often the validation images will be outputted

logging_params:
  # parameters for logging modules

  save_dir: optional logs/
    # where the logger's results are saved
  name: optional VAE3D32
    # name of the folder of logging results
  visualize_interval: optional 50 
    # the interval for outputing visualzation images
    # NOTE: has to be larger than "check_val_every_n_epoch"
    # best to be multiplications of it.
  manual_seed: optional 9001

file_name: optional
  # filename of this config file

freeze_params: optional
  # e.g. encoder: [0, 1]